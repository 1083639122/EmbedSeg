{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from EmbedSeg.train import begin_training\n",
    "from EmbedSeg.utils.create_dicts import create_dataset_dict, create_model_dict, create_loss_dict, create_configs\n",
    "import torch\n",
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the path to `train`, `val` crops and the type of `center` embedding which we would like to train the network for:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train-val images, masks and center-images will be accessed from the path specified by `data_dir` and `project-name`.\n",
    "<a id='center'></a>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'crops'\n",
    "project_name = 'bbbc010-2012'\n",
    "center = 'medoid'\n",
    "\n",
    "print(\"Project Name chosen as : {}. \\nTrain-Val images-masks-center-images will be accessed from : {}\".format(project_name, data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert center in {'medoid', 'approximate-medoid', 'centroid'}\n",
    "    print(\"Spatial Embedding Location chosen as : {}\".format(center))\n",
    "except AssertionError as e:\n",
    "    e.args += ('Please specify center as one of : {\"medoid\", \"approximate-medoid\", \"centroid\"}', 42)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify training dataset-related parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hints: \n",
    "* The `train_size` attribute indicates the number of image-mask paired examples which the network would see in one complete epoch. Ideally this should be the number of `train` image crops. For the `bbbc010-2012` dataset, we obtain ~600 crops, so we set `train_size` to 600. \n",
    "* The effective batch size is determined as a product of the attributes `train_batch_size` and `virtual_train_batch_multiplier`. For one-hot encoded instances (such as the `bbbc010-2012` dataset), **`train_batch_size` should be set equal to 1**. \n",
    "* The `normalization_factor` attribute normalizes the raw images to always be between 0 and 1. For 8- bit images, please set `normalization_factor = 255`, for 16-bit images, please set `normalization_factor = 65535`.\n",
    "* The `one_hot` attribute should be set to True if the instance image is present in an one-hot encoded style (i.e. object instance is encoded as 1 in its own individual image slice) and False if the instance image is the same dimensions as the raw-image. \n",
    "\n",
    "In the cell after this one, a `train_dataset_dict` dictionary is generated from the parameters specified here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 600\n",
    "train_batch_size = 1 \n",
    "virtual_train_batch_multiplier = 1 \n",
    "normalization_factor = 65535\n",
    "one_hot = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the `train_dataset_dict` dictionary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_dict = create_dataset_dict(data_dir = data_dir, \n",
    "                                         project_name = project_name,  \n",
    "                                         center = center, \n",
    "                                         size = train_size, \n",
    "                                         batch_size = train_batch_size, \n",
    "                                         virtual_batch_multiplier = virtual_train_batch_multiplier, \n",
    "                                         normalization_factor= normalization_factor,\n",
    "                                         one_hot = one_hot,\n",
    "                                         type = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify validation dataset-related parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hints:\n",
    "* The size attribute indicates the number of image-mask paired examples which the network would see in one complete epoch. Here, it is recommended to set `val_size` equal to an integral multiple of total number of validation image crops. For example, for the `bbbc010-2012` dataset, we notice ~100 validation crops, but we set `val_size = 800` to obtain the optimal results across the 8-fold augmentation of these 100 crops. It would also be fine to set `val_size = 100`.\n",
    "* The effective batch size is determined as a product of the attributes `val_batch_size` and `virtual_val_batch_multiplier`. For one-hot encoded instances (such as the `bbbc010-2012` dataset), **`val_batch_size` should be set equal to 1**. \n",
    "* The `normalization_factor` attribute normalizes the raw images to always be between 0 and 1. For 8- bit images, please set `normalization_factor = 255`, for 16-bit images, please set `normalization_factor = 65535`. Please note that the `normalization_factor` should be the same for both train and validation images.\n",
    "* The `one_hot` attribute should be set to True if the instance image is present in an one-hot encoded style (i.e. object instance is encoded as 1 in its own individual image slice) and False if the instance image is the same dimensions as the raw-image. \n",
    "\n",
    "In the cell after this one, a `val_dataset_dict` dictionary is generated from the parameters specified here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 800\n",
    "val_batch_size = 1\n",
    "virtual_val_batch_multiplier = 1\n",
    "normalization_factor = 65535\n",
    "one_hot = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the `val_dataset_dict` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_dict = create_dataset_dict(data_dir = data_dir, \n",
    "                                       project_name = project_name, \n",
    "                                       center = center, \n",
    "                                       size = val_size, \n",
    "                                       batch_size = val_batch_size, \n",
    "                                       virtual_batch_multiplier = virtual_val_batch_multiplier,\n",
    "                                       normalization_factor= normalization_factor,\n",
    "                                       one_hot = one_hot,\n",
    "                                       type ='val',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify model-related parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hints:\n",
    "* Set the `input_channels` attribute equal to the number of channels in the input images. \n",
    "\n",
    "In the cell after this one, a `model_dataset_dict` dictionary is generated from the parameters specified here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the `model_dict` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = create_model_dict(input_channels = input_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the `loss_dict` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict = create_loss_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify additional parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hints:\n",
    "* The `n_epochs` attribute determines how long the training should proceed. In general for good results on `bbbbc_010` dataset with the configurations above, you should train for longer than 100 epochs.\n",
    "* The `display` attribute, if set to True, allows you to see the network predictions as the training proceeds. \n",
    "* The `display_embedding` attribute, if set to True, allows you to see some sample embedding as the training proceeds. Setting this to False leads to faster training times.\n",
    "* The `save_dir` attribute identifies the location where the checkpoints and loss curve details are saved. \n",
    "* If one wishes to **resume training** from a previous checkpoint, they could point `resume_path` attribute appropriately. For example, one could set `resume_path = './experiment/Jan-24-2021-bbbc010-2012/checkpoint.pth'` to resume training from the last checkpoint. \n",
    "* The `grid_y` and `grid_x` attributes should be set to equal or more than the dimensions of the largest evaluation image one wishes to test the trained model on. (Here, we assume that the pixel sizes in the height and width dimension are equal). If you are unsure of the evaluation image sizes at this stage, best to leave these attributes as they are.  \n",
    "* The `one_hot` attribute should be set to True if the instance image is present in an one-hot encoded style (i.e. object instance is encoded as 1 in its own individual image slice) and False if the instance image is the same dimensions as the raw-image. \n",
    "\n",
    "\n",
    "\n",
    "In the cell after this one, a `configs` dictionary is generated from the parameters specified here!\n",
    "<a id='resume'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "one_hot = True\n",
    "display = True\n",
    "display_embedding = False\n",
    "save_dir = os.path.join('experiment', project_name+'-'+'demo')\n",
    "resume_path  = None\n",
    "grid_y = 1024\n",
    "grid_x = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the  `configs` dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = create_configs(n_epochs = n_epochs,\n",
    "                         one_hot = one_hot,\n",
    "                         display = display, \n",
    "                         display_embedding = display_embedding,\n",
    "                         resume_path = resume_path, \n",
    "                         save_dir = save_dir, \n",
    "                         grid_y = grid_y, \n",
    "                         grid_x = grid_x,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the next cell would begin the training. \n",
    "\n",
    "If `display` attribute was set to `True` above, then you would see the network predictions at every $n^{th}$ step (equals 5, by default) on training and validation images. \n",
    "\n",
    "Going clockwise from top-left is \n",
    "\n",
    "    * the raw-image which needs to be segmented, \n",
    "    * the corresponding ground truth instance mask, \n",
    "    * the network predicted instance mask, and \n",
    "    * (if display_embedding = True) from each object instance, 5 pixels are randomly selected (indicated with `+`), their embeddings are plotted (indicated with `.`) and the predicted margin for that object is visualized as an axis-aligned ellipse centred on the ground-truth - center (indicated with `x`)  for that object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_training(train_dataset_dict, val_dataset_dict, model_dict, loss_dict, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "  Common causes for errors during training, may include : <br>\n",
    "    1. Not having <b>center images</b> for  <b>both</b> train and val directories  <br>\n",
    "    2. <b>Mismatch</b> between type of center-images saved in <b>01-data.ipynb</b> and the type of center chosen in this notebook (see the <b><a href=\"#center\"> center</a></b> parameter in the third code cell in this notebook)   <br>\n",
    "    3. In case of resuming training from a previous checkpoint, please ensure that the model weights are read from the correct directory, using the <b><a href=\"#resume\"> resume_path</a></b> parameter. Additionally, please ensure that the <b>save_dir</b> parameter for saving the model weights points to a relevant directory. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EmbedSegEnv",
   "language": "python",
   "name": "embedsegenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "47.7167px",
    "left": "1689.05px",
    "top": "111.133px",
    "width": "158.95px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
